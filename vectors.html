<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>vectors API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>vectors</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from sklearn.neighbors import NearestNeighbors
import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors
from sklearn.decomposition import PCA

import spacy
nlp = spacy.load(&#34;en_core_web_lg&#34;)

class Notes(object):
    &#34;&#34;&#34;
    Some important notes from lecture.
    &#34;&#34;&#34;

    @staticmethod
    def test():
        return &#39;test&#39;

    @staticmethod
    def tokenize(document):
        &#34;&#34;&#34;
        input document

        :returns a tokenized lemma list
        &#34;&#34;&#34;
        doc = nlp(document)
        return [token.lemma_ for token in doc if (token.is_stop != True) and (token.is_punct != True)]

    @staticmethod
    def gather_data(filefolder):
        &#34;&#34;&#34; Produces List of Documents from a Directory

        filefolder (str): a path of .txt files

        :returns list of strings
        &#34;&#34;&#34;

        data = []

        files = os.listdir(filefolder)  # Causes variation across machines

        for article in files:

            path = os.path.join(filefolder, article)

            if path[-3:] == &#39;txt&#39;:  # os ~endswith(&#39;txt&#39;)
                with open(path, &#39;rb&#39;) as f:
                    data.append(f.read())

        return data

    @staticmethod
    def vectorize(text2fit, test2tranform):
        &#34;&#34;&#34;
        input text to fit and text to transform

        :returns a vectorized dtm
        &#34;&#34;&#34;
        from sklearn.feature_extraction.text import CountVectorizer
        vect = CountVectorizer()
        vect.fit(text2fit)
        dtm = vect.transform(test2tranform)
        return dtm

    @staticmethod
    def dtm_word_count(vectorize, vect, text_in):
        &#34;&#34;&#34;
        inputs vectorize model and vect for column,

        :return: word count.
        &#34;&#34;&#34;
        dtm = vectorize(text_in)
        output = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())
        return output

    @staticmethod
    def distribution_plot(data):
        &#34;&#34;&#34;
        Inputs dataframe to visualize our distribution
        &#34;&#34;&#34;
        doc_len = [len(doc) for doc in data]
        import seaborn as sns

        return sns.distplot(doc_len)

    @staticmethod
    def Term_frequency(data):
        &#34;&#34;&#34;
        Percentage of words in a document

        Document Frequency: A penalty for the word existing in a high number of documents.

        &#34;&#34;&#34;
        from sklearn.feature_extraction.text import TfidfVectorizer

        # Instantiate vectorizer object
        tfidf = TfidfVectorizer(stop_words=&#39;english&#39;, max_features=5000)

        # Create a vocabulary and get word counts per document
        # Similiar to fit_predict
        dtm = tfidf.fit_transform(data)

        # Print word counts

        # Get feature names to use as dataframe column headers
        dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())

        return dtm

    @staticmethod
    def cosine_sim(dtm):
        &#34;&#34;&#34;
        import dtm
        Calculate Distance of TF-IDF Vectors

        &#34;&#34;&#34;
        from sklearn.metrics.pairwise import cosine_similarity

        dist_matrix = cosine_similarity(dtm)
        df = pd.DataFrame(dist_matrix)
        return df


class KnnState(object):
    from sklearn.neighbors import NearestNeighbors
    &#34;&#34;&#34;
    Used to save our NearestNeighbors model
    &#34;&#34;&#34;
    def __init__(self):
        self._nn = NearestNeighbors(n_neighbors=5, algorithm=&#39;kd_tree&#39;)

    @property
    def test(self):
        return self._nn

    @test.setter
    def test(self, value):
        self._nn = value


class Knn(object):
    @staticmethod
    def fit(nn, dtm):
        &#34;&#34;&#34;fits our model&#34;&#34;&#34;
        return nn.fit(dtm)

    @staticmethod
    def values(nn, dtm, row=0):
        &#34;&#34;&#34;shows values of our model&#34;&#34;&#34;
        return nn.kneighbors([dtm.iloc[row].values])

    @staticmethod
    def query(nn, dtm, row=256):
        &#34;&#34;&#34;Query a specific row from a dtm&#34;&#34;&#34;
        return nn.kneighbors([dtm.iloc[row]])

    @staticmethod
    def density(nn, new):
        &#34;&#34;&#34;Check density&#34;&#34;&#34;
        nn.kneighbors(new.todense())


class Word2Vec(object):
    from sklearn.decomposition import PCA
    @staticmethod
    def similarity(a, b):
        &#34;&#34;&#34;
        Input two nlp() classes to determine similarity

        &#34;&#34;&#34;
        similarity = a.similarity(b)
        return similarity

    @staticmethod
    def get_word_vectors(words):
        &#34;&#34;&#34;
        converts a list of words into their word vectors
        &#34;&#34;&#34;
        return [nlp(word).vector for word in words]

    @staticmethod
    def process_PCA(get_word_vectors):
        &#34;&#34;&#34;
        intialise pca model and tell it to project data down onto 2 dimensions

        fit the pca model to our 300D data, this will work out which is the best
        way to project the data down that will best maintain the relative distances
        between data points. It will store these intructioons on how to transform the data.

        Tell our (fitted) pca model to transform our 300D data down onto 2D using the
        instructions it learnt during the fit phase.

        let&#39;s look at our new 2D word vectors
        &#34;&#34;&#34;
        words = [&#39;car&#39;, &#39;truck&#39;, &#39;suv&#39;, &#39;race&#39;, &#39;elves&#39;, &#39;dragon&#39;, &#39;sword&#39;, &#39;king&#39;, &#39;queen&#39;, &#39;prince&#39;, &#39;horse&#39;, &#39;fish&#39;,
                 &#39;lion&#39;, &#39;tiger&#39;, &#39;lynx&#39;, &#39;potato&#39;]

        # intialise pca model and tell it to project data down onto 2 dimensions
        pca = PCA(n_components=2)

        # fit the pca model to our 300D data, this will work out which is the best
        # way to project the data down that will best maintain the relative distances
        # between data points. It will store these intructioons on how to transform the data.
        pca.fit(get_word_vectors(words))

        # Tell our (fitted) pca model to transform our 300D data down onto 2D using the
        # instructions it learnt during the fit phase.
        word_vecs_2d = pca.transform(get_word_vectors(words))

        # let&#39;s look at our new 2D word vectors
        return word_vecs_2d</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="vectors.Knn"><code class="flex name class">
<span>class <span class="ident">Knn</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Knn(object):
    @staticmethod
    def fit(nn, dtm):
        &#34;&#34;&#34;fits our model&#34;&#34;&#34;
        return nn.fit(dtm)

    @staticmethod
    def values(nn, dtm, row=0):
        &#34;&#34;&#34;shows values of our model&#34;&#34;&#34;
        return nn.kneighbors([dtm.iloc[row].values])

    @staticmethod
    def query(nn, dtm, row=256):
        &#34;&#34;&#34;Query a specific row from a dtm&#34;&#34;&#34;
        return nn.kneighbors([dtm.iloc[row]])

    @staticmethod
    def density(nn, new):
        &#34;&#34;&#34;Check density&#34;&#34;&#34;
        nn.kneighbors(new.todense())</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="vectors.Knn.density"><code class="name flex">
<span>def <span class="ident">density</span></span>(<span>nn, new)</span>
</code></dt>
<dd>
<div class="desc"><p>Check density</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def density(nn, new):
    &#34;&#34;&#34;Check density&#34;&#34;&#34;
    nn.kneighbors(new.todense())</code></pre>
</details>
</dd>
<dt id="vectors.Knn.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>nn, dtm)</span>
</code></dt>
<dd>
<div class="desc"><p>fits our model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def fit(nn, dtm):
    &#34;&#34;&#34;fits our model&#34;&#34;&#34;
    return nn.fit(dtm)</code></pre>
</details>
</dd>
<dt id="vectors.Knn.query"><code class="name flex">
<span>def <span class="ident">query</span></span>(<span>nn, dtm, row=256)</span>
</code></dt>
<dd>
<div class="desc"><p>Query a specific row from a dtm</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def query(nn, dtm, row=256):
    &#34;&#34;&#34;Query a specific row from a dtm&#34;&#34;&#34;
    return nn.kneighbors([dtm.iloc[row]])</code></pre>
</details>
</dd>
<dt id="vectors.Knn.values"><code class="name flex">
<span>def <span class="ident">values</span></span>(<span>nn, dtm, row=0)</span>
</code></dt>
<dd>
<div class="desc"><p>shows values of our model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def values(nn, dtm, row=0):
    &#34;&#34;&#34;shows values of our model&#34;&#34;&#34;
    return nn.kneighbors([dtm.iloc[row].values])</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="vectors.KnnState"><code class="flex name class">
<span>class <span class="ident">KnnState</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class KnnState(object):
    from sklearn.neighbors import NearestNeighbors
    &#34;&#34;&#34;
    Used to save our NearestNeighbors model
    &#34;&#34;&#34;
    def __init__(self):
        self._nn = NearestNeighbors(n_neighbors=5, algorithm=&#39;kd_tree&#39;)

    @property
    def test(self):
        return self._nn

    @test.setter
    def test(self, value):
        self._nn = value</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="vectors.KnnState.NearestNeighbors"><code class="name">var <span class="ident">NearestNeighbors</span></code></dt>
<dd>
<div class="desc"><p>Unsupervised learner for implementing neighbor searches.</p>
<p>Read more in the :ref:<code>User Guide &lt;unsupervised_neighbors&gt;</code>.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.9</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_neighbors</code></strong> :&ensp;<code>int</code>, default=<code>5</code></dt>
<dd>Number of neighbors to use by default for :meth:<code>kneighbors</code> queries.</dd>
<dt><strong><code>radius</code></strong> :&ensp;<code>float</code>, default=<code>1.0</code></dt>
<dd>Range of parameter space to use by default for :meth:<code>radius_neighbors</code>
queries.</dd>
<dt><strong><code>algorithm</code></strong> :&ensp;<code>{'auto', 'ball_tree', 'kd_tree', 'brute'}</code>, default=<code>'auto'</code></dt>
<dd>
<p>Algorithm used to compute the nearest neighbors:</p>
<ul>
<li>'ball_tree' will use :class:<code>BallTree</code></li>
<li>'kd_tree' will use :class:<code>KDTree</code></li>
<li>'brute' will use a brute-force search.</li>
<li>'auto' will attempt to decide the most appropriate algorithm
based on the values passed to :meth:<code>fit</code> method.</li>
</ul>
<p>Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt><strong><code>leaf_size</code></strong> :&ensp;<code>int</code>, default=<code>30</code></dt>
<dd>Leaf size passed to BallTree or KDTree.
This can affect the
speed of the construction and query, as well as the memory
required to store the tree.
The optimal value depends on the
nature of the problem.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>str</code> or <code>callable</code>, default=<code>'minkowski'</code></dt>
<dd>the distance metric to use for the tree.
The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of :class:<code>DistanceMetric</code> for a
list of available metrics.
If metric is "precomputed", X is assumed to be a distance matrix and
must be square during fit. X may be a :term:<code>sparse graph</code>,
in which case only "nonzero" elements may be considered neighbors.</dd>
<dt><strong><code>p</code></strong> :&ensp;<code>int</code>, default=<code>2</code></dt>
<dd>Parameter for the Minkowski metric from
sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</dd>
<dt><strong><code>metric_params</code></strong> :&ensp;<code>dict</code>, default=<code>None</code></dt>
<dd>Additional keyword arguments for the metric function.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code>, default=<code>None</code></dt>
<dd>The number of parallel jobs to run for neighbors search.
<code>None</code> means 1 unless in a :obj:<code>joblib.parallel_backend</code> context.
<code>-1</code> means using all processors. See :term:<code>Glossary &lt;n_jobs&gt;</code>
for more details.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>effective_metric_</code></strong> :&ensp;<code>str</code></dt>
<dd>Metric used to compute distances to neighbors.</dd>
<dt><strong><code>effective_metric_params_</code></strong> :&ensp;<code>dict</code></dt>
<dd>Parameters for the metric used to compute distances to neighbors.</dd>
</dl>
<h2 id="examples">Examples</h2>
<blockquote>
<blockquote>
<blockquote>
<p>import numpy as np
from sklearn.neighbors import NearestNeighbors
samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]</p>
<p>neigh = NearestNeighbors(n_neighbors=2, radius=0.4)
neigh.fit(samples)
NearestNeighbors(&hellip;)</p>
<p>neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)
array([[2, 0]]&hellip;)</p>
<p>nbrs = neigh.radius_neighbors([[0, 0, 1.3]], 0.4, return_distance=False)
np.asarray(nbrs[0][0])
array(2)</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="see-also">See Also</h2>
<p><code>KNeighborsClassifier</code>
<code>RadiusNeighborsClassifier</code>
<code>KNeighborsRegressor</code>
<code>RadiusNeighborsRegressor</code>
<code>BallTree</code></p>
<h2 id="notes">Notes</h2>
<p>See :ref:<code>Nearest Neighbors &lt;neighbors&gt;</code> in the online documentation
for a discussion of the choice of <code>algorithm</code> and <code>leaf_size</code>.</p>
<p><a href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="vectors.KnnState.test"><code class="name">var <span class="ident">test</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def test(self):
    return self._nn</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="vectors.Notes"><code class="flex name class">
<span>class <span class="ident">Notes</span></span>
</code></dt>
<dd>
<div class="desc"><p>Some important notes from lecture.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Notes(object):
    &#34;&#34;&#34;
    Some important notes from lecture.
    &#34;&#34;&#34;

    @staticmethod
    def test():
        return &#39;test&#39;

    @staticmethod
    def tokenize(document):
        &#34;&#34;&#34;
        input document

        :returns a tokenized lemma list
        &#34;&#34;&#34;
        doc = nlp(document)
        return [token.lemma_ for token in doc if (token.is_stop != True) and (token.is_punct != True)]

    @staticmethod
    def gather_data(filefolder):
        &#34;&#34;&#34; Produces List of Documents from a Directory

        filefolder (str): a path of .txt files

        :returns list of strings
        &#34;&#34;&#34;

        data = []

        files = os.listdir(filefolder)  # Causes variation across machines

        for article in files:

            path = os.path.join(filefolder, article)

            if path[-3:] == &#39;txt&#39;:  # os ~endswith(&#39;txt&#39;)
                with open(path, &#39;rb&#39;) as f:
                    data.append(f.read())

        return data

    @staticmethod
    def vectorize(text2fit, test2tranform):
        &#34;&#34;&#34;
        input text to fit and text to transform

        :returns a vectorized dtm
        &#34;&#34;&#34;
        from sklearn.feature_extraction.text import CountVectorizer
        vect = CountVectorizer()
        vect.fit(text2fit)
        dtm = vect.transform(test2tranform)
        return dtm

    @staticmethod
    def dtm_word_count(vectorize, vect, text_in):
        &#34;&#34;&#34;
        inputs vectorize model and vect for column,

        :return: word count.
        &#34;&#34;&#34;
        dtm = vectorize(text_in)
        output = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())
        return output

    @staticmethod
    def distribution_plot(data):
        &#34;&#34;&#34;
        Inputs dataframe to visualize our distribution
        &#34;&#34;&#34;
        doc_len = [len(doc) for doc in data]
        import seaborn as sns

        return sns.distplot(doc_len)

    @staticmethod
    def Term_frequency(data):
        &#34;&#34;&#34;
        Percentage of words in a document

        Document Frequency: A penalty for the word existing in a high number of documents.

        &#34;&#34;&#34;
        from sklearn.feature_extraction.text import TfidfVectorizer

        # Instantiate vectorizer object
        tfidf = TfidfVectorizer(stop_words=&#39;english&#39;, max_features=5000)

        # Create a vocabulary and get word counts per document
        # Similiar to fit_predict
        dtm = tfidf.fit_transform(data)

        # Print word counts

        # Get feature names to use as dataframe column headers
        dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())

        return dtm

    @staticmethod
    def cosine_sim(dtm):
        &#34;&#34;&#34;
        import dtm
        Calculate Distance of TF-IDF Vectors

        &#34;&#34;&#34;
        from sklearn.metrics.pairwise import cosine_similarity

        dist_matrix = cosine_similarity(dtm)
        df = pd.DataFrame(dist_matrix)
        return df</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="vectors.Notes.Term_frequency"><code class="name flex">
<span>def <span class="ident">Term_frequency</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"><p>Percentage of words in a document</p>
<p>Document Frequency: A penalty for the word existing in a high number of documents.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def Term_frequency(data):
    &#34;&#34;&#34;
    Percentage of words in a document

    Document Frequency: A penalty for the word existing in a high number of documents.

    &#34;&#34;&#34;
    from sklearn.feature_extraction.text import TfidfVectorizer

    # Instantiate vectorizer object
    tfidf = TfidfVectorizer(stop_words=&#39;english&#39;, max_features=5000)

    # Create a vocabulary and get word counts per document
    # Similiar to fit_predict
    dtm = tfidf.fit_transform(data)

    # Print word counts

    # Get feature names to use as dataframe column headers
    dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())

    return dtm</code></pre>
</details>
</dd>
<dt id="vectors.Notes.cosine_sim"><code class="name flex">
<span>def <span class="ident">cosine_sim</span></span>(<span>dtm)</span>
</code></dt>
<dd>
<div class="desc"><p>import dtm
Calculate Distance of TF-IDF Vectors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def cosine_sim(dtm):
    &#34;&#34;&#34;
    import dtm
    Calculate Distance of TF-IDF Vectors

    &#34;&#34;&#34;
    from sklearn.metrics.pairwise import cosine_similarity

    dist_matrix = cosine_similarity(dtm)
    df = pd.DataFrame(dist_matrix)
    return df</code></pre>
</details>
</dd>
<dt id="vectors.Notes.distribution_plot"><code class="name flex">
<span>def <span class="ident">distribution_plot</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"><p>Inputs dataframe to visualize our distribution</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def distribution_plot(data):
    &#34;&#34;&#34;
    Inputs dataframe to visualize our distribution
    &#34;&#34;&#34;
    doc_len = [len(doc) for doc in data]
    import seaborn as sns

    return sns.distplot(doc_len)</code></pre>
</details>
</dd>
<dt id="vectors.Notes.dtm_word_count"><code class="name flex">
<span>def <span class="ident">dtm_word_count</span></span>(<span>vectorize, vect, text_in)</span>
</code></dt>
<dd>
<div class="desc"><p>inputs vectorize model and vect for column,</p>
<p>:return: word count.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def dtm_word_count(vectorize, vect, text_in):
    &#34;&#34;&#34;
    inputs vectorize model and vect for column,

    :return: word count.
    &#34;&#34;&#34;
    dtm = vectorize(text_in)
    output = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())
    return output</code></pre>
</details>
</dd>
<dt id="vectors.Notes.gather_data"><code class="name flex">
<span>def <span class="ident">gather_data</span></span>(<span>filefolder)</span>
</code></dt>
<dd>
<div class="desc"><p>Produces List of Documents from a Directory</p>
<p>filefolder (str): a path of .txt files</p>
<p>:returns list of strings</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def gather_data(filefolder):
    &#34;&#34;&#34; Produces List of Documents from a Directory

    filefolder (str): a path of .txt files

    :returns list of strings
    &#34;&#34;&#34;

    data = []

    files = os.listdir(filefolder)  # Causes variation across machines

    for article in files:

        path = os.path.join(filefolder, article)

        if path[-3:] == &#39;txt&#39;:  # os ~endswith(&#39;txt&#39;)
            with open(path, &#39;rb&#39;) as f:
                data.append(f.read())

    return data</code></pre>
</details>
</dd>
<dt id="vectors.Notes.test"><code class="name flex">
<span>def <span class="ident">test</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def test():
    return &#39;test&#39;</code></pre>
</details>
</dd>
<dt id="vectors.Notes.tokenize"><code class="name flex">
<span>def <span class="ident">tokenize</span></span>(<span>document)</span>
</code></dt>
<dd>
<div class="desc"><p>input document</p>
<p>:returns a tokenized lemma list</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def tokenize(document):
    &#34;&#34;&#34;
    input document

    :returns a tokenized lemma list
    &#34;&#34;&#34;
    doc = nlp(document)
    return [token.lemma_ for token in doc if (token.is_stop != True) and (token.is_punct != True)]</code></pre>
</details>
</dd>
<dt id="vectors.Notes.vectorize"><code class="name flex">
<span>def <span class="ident">vectorize</span></span>(<span>text2fit, test2tranform)</span>
</code></dt>
<dd>
<div class="desc"><p>input text to fit and text to transform</p>
<p>:returns a vectorized dtm</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def vectorize(text2fit, test2tranform):
    &#34;&#34;&#34;
    input text to fit and text to transform

    :returns a vectorized dtm
    &#34;&#34;&#34;
    from sklearn.feature_extraction.text import CountVectorizer
    vect = CountVectorizer()
    vect.fit(text2fit)
    dtm = vect.transform(test2tranform)
    return dtm</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="vectors.Word2Vec"><code class="flex name class">
<span>class <span class="ident">Word2Vec</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Word2Vec(object):
    from sklearn.decomposition import PCA
    @staticmethod
    def similarity(a, b):
        &#34;&#34;&#34;
        Input two nlp() classes to determine similarity

        &#34;&#34;&#34;
        similarity = a.similarity(b)
        return similarity

    @staticmethod
    def get_word_vectors(words):
        &#34;&#34;&#34;
        converts a list of words into their word vectors
        &#34;&#34;&#34;
        return [nlp(word).vector for word in words]

    @staticmethod
    def process_PCA(get_word_vectors):
        &#34;&#34;&#34;
        intialise pca model and tell it to project data down onto 2 dimensions

        fit the pca model to our 300D data, this will work out which is the best
        way to project the data down that will best maintain the relative distances
        between data points. It will store these intructioons on how to transform the data.

        Tell our (fitted) pca model to transform our 300D data down onto 2D using the
        instructions it learnt during the fit phase.

        let&#39;s look at our new 2D word vectors
        &#34;&#34;&#34;
        words = [&#39;car&#39;, &#39;truck&#39;, &#39;suv&#39;, &#39;race&#39;, &#39;elves&#39;, &#39;dragon&#39;, &#39;sword&#39;, &#39;king&#39;, &#39;queen&#39;, &#39;prince&#39;, &#39;horse&#39;, &#39;fish&#39;,
                 &#39;lion&#39;, &#39;tiger&#39;, &#39;lynx&#39;, &#39;potato&#39;]

        # intialise pca model and tell it to project data down onto 2 dimensions
        pca = PCA(n_components=2)

        # fit the pca model to our 300D data, this will work out which is the best
        # way to project the data down that will best maintain the relative distances
        # between data points. It will store these intructioons on how to transform the data.
        pca.fit(get_word_vectors(words))

        # Tell our (fitted) pca model to transform our 300D data down onto 2D using the
        # instructions it learnt during the fit phase.
        word_vecs_2d = pca.transform(get_word_vectors(words))

        # let&#39;s look at our new 2D word vectors
        return word_vecs_2d</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="vectors.Word2Vec.PCA"><code class="name">var <span class="ident">PCA</span></code></dt>
<dd>
<div class="desc"><p>Principal component analysis (PCA).</p>
<p>Linear dimensionality reduction using Singular Value Decomposition of the
data to project it to a lower dimensional space. The input data is centered
but not scaled for each feature before applying the SVD.</p>
<p>It uses the LAPACK implementation of the full SVD or a randomized truncated
SVD by the method of Halko et al. 2009, depending on the shape of the input
data and the number of components to extract.</p>
<p>It can also use the scipy.sparse.linalg ARPACK implementation of the
truncated SVD.</p>
<p>Notice that this class does not support sparse input. See
:class:<code>TruncatedSVD</code> for an alternative with sparse data.</p>
<p>Read more in the :ref:<code>User Guide &lt;PCA&gt;</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_components</code></strong> :&ensp;<code>int, float, None</code> or <code>str</code></dt>
<dd>
<p>Number of components to keep.
if n_components is not set all components are kept::</p>
<pre><code>n_components == min(n_samples, n_features)
</code></pre>
<p>If <code>n_components == 'mle'</code> and <code>svd_solver == 'full'</code>, Minka's
MLE is used to guess the dimension. Use of <code>n_components == 'mle'</code>
will interpret <code>svd_solver == 'auto'</code> as <code>svd_solver == 'full'</code>.</p>
<p>If <code>0 &lt; n_components &lt; 1</code> and <code>svd_solver == 'full'</code>, select the
number of components such that the amount of variance that needs to be
explained is greater than the percentage specified by n_components.</p>
<p>If <code>svd_solver == 'arpack'</code>, the number of components must be
strictly less than the minimum of n_features and n_samples.</p>
<p>Hence, the None case results in::</p>
<pre><code>n_components == min(n_samples, n_features) - 1
</code></pre>
</dd>
<dt><strong><code>copy</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>If False, data passed to fit are overwritten and running
fit(X).transform(X) will not yield the expected results,
use fit_transform(X) instead.</dd>
<dt><strong><code>whiten</code></strong> :&ensp;<code>bool</code>, optional <code>(default False)</code></dt>
<dd>
<p>When True (False by default) the <code>components_</code> vectors are multiplied
by the square root of n_samples and then divided by the singular values
to ensure uncorrelated outputs with unit component-wise variances.</p>
<p>Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometime
improve the predictive accuracy of the downstream estimators by
making their data respect some hard-wired assumptions.</p>
</dd>
<dt><strong><code>svd_solver</code></strong> :&ensp;<code>str {'auto', 'full', 'arpack', 'randomized'}</code></dt>
<dd>
<p>If auto :
The solver is selected by a default policy based on <code>X.shape</code> and
<code>n_components</code>: if the input data is larger than 500x500 and the
number of components to extract is lower than 80% of the smallest
dimension of the data, then the more efficient 'randomized'
method is enabled. Otherwise the exact full SVD is computed and
optionally truncated afterwards.
If full :
run exact full SVD calling the standard LAPACK solver via
<code>scipy.linalg.svd</code> and select the components by postprocessing
If arpack :
run SVD truncated to n_components calling ARPACK solver via
<code>scipy.sparse.linalg.svds</code>. It requires strictly
0 &lt; n_components &lt; min(X.shape)
If randomized :
run randomized SVD by the method of Halko et al.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.18.0</p>
</div>
</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float &gt;= 0</code>, optional <code>(default .0)</code></dt>
<dd>
<p>Tolerance for singular values computed by svd_solver == 'arpack'.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.18.0</p>
</div>
</dd>
<dt><strong><code>iterated_power</code></strong> :&ensp;<code>int &gt;= 0,</code> or <code>'auto', (default 'auto')</code></dt>
<dd>
<p>Number of iterations for the power method computed by
svd_solver == 'randomized'.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.18.0</p>
</div>
</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int, RandomState instance</code>, default=<code>None</code></dt>
<dd>
<p>Used when <code>svd_solver</code> == 'arpack' or 'randomized'. Pass an int
for reproducible results across multiple function calls.
See :term:<code>Glossary &lt;random_state&gt;</code>.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.18.0</p>
</div>
</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>components_</code></strong> :&ensp;<code>array, shape (n_components, n_features)</code></dt>
<dd>Principal axes in feature space, representing the directions of
maximum variance in the data. The components are sorted by
<code>explained_variance_</code>.</dd>
<dt><strong><code>explained_variance_</code></strong> :&ensp;<code>array, shape (n_components,)</code></dt>
<dd>
<p>The amount of variance explained by each of the selected components.</p>
<p>Equal to n_components largest eigenvalues
of the covariance matrix of X.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.18</p>
</div>
</dd>
<dt><strong><code>explained_variance_ratio_</code></strong> :&ensp;<code>array, shape (n_components,)</code></dt>
<dd>
<p>Percentage of variance explained by each of the selected components.</p>
<p>If <code>n_components</code> is not set then all components are stored and the
sum of the ratios is equal to 1.0.</p>
</dd>
<dt><strong><code>singular_values_</code></strong> :&ensp;<code>array, shape (n_components,)</code></dt>
<dd>
<p>The singular values corresponding to each of the selected components.
The singular values are equal to the 2-norms of the <code>n_components</code>
variables in the lower-dimensional space.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.19</p>
</div>
</dd>
<dt><strong><code>mean_</code></strong> :&ensp;<code>array, shape (n_features,)</code></dt>
<dd>
<p>Per-feature empirical mean, estimated from the training set.</p>
<p>Equal to <code>X.mean(axis=0)</code>.</p>
</dd>
<dt><strong><code>n_components_</code></strong> :&ensp;<code>int</code></dt>
<dd>The estimated number of components. When n_components is set
to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this
number is estimated from input data. Otherwise it equals the parameter
n_components, or the lesser value of n_features and n_samples
if n_components is None.</dd>
<dt><strong><code>n_features_</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of features in the training data.</dd>
<dt><strong><code>n_samples_</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of samples in the training data.</dd>
<dt><strong><code>noise_variance_</code></strong> :&ensp;<code>float</code></dt>
<dd>
<p>The estimated noise covariance following the Probabilistic PCA model
from Tipping and Bishop 1999. See "Pattern Recognition and
Machine Learning" by C. Bishop, 12.2.1 p. 574 or
<a href="http://www.miketipping.com/papers/met-mppca.pdf.">http://www.miketipping.com/papers/met-mppca.pdf.</a> It is required to
compute the estimated data covariance and score samples.</p>
<p>Equal to the average of (min(n_features, n_samples) - n_components)
smallest eigenvalues of the covariance matrix of X.</p>
</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>KernelPCA</code></dt>
<dd>Kernel Principal Component Analysis.</dd>
<dt><code>SparsePCA</code></dt>
<dd>Sparse Principal Component Analysis.</dd>
<dt><code>TruncatedSVD</code></dt>
<dd>Dimensionality reduction using truncated SVD.</dd>
<dt><code>IncrementalPCA</code></dt>
<dd>Incremental Principal Component Analysis.</dd>
</dl>
<h2 id="references">References</h2>
<p>For n_components == 'mle', this class uses the method of <em>Minka, T. P.
"Automatic choice of dimensionality for PCA". In NIPS, pp. 598-604</em></p>
<p>Implements the probabilistic PCA model from:
Tipping, M. E., and Bishop, C. M. (1999). "Probabilistic principal
component analysis". Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 61(3), 611-622.
via the score and score_samples methods.
See <a href="http://www.miketipping.com/papers/met-mppca.pdf">http://www.miketipping.com/papers/met-mppca.pdf</a></p>
<p>For svd_solver == 'arpack', refer to <code>scipy.sparse.linalg.svds</code>.</p>
<p>For svd_solver == 'randomized', see:
<em>Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).
"Finding structure with randomness: Probabilistic algorithms for
constructing approximate matrix decompositions".
SIAM review, 53(2), 217-288.</em> and also
<em>Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).
"A randomized algorithm for the decomposition of matrices".
Applied and Computational Harmonic Analysis, 30(1), 47-68.</em></p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.decomposition import PCA
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
&gt;&gt;&gt; pca = PCA(n_components=2)
&gt;&gt;&gt; pca.fit(X)
PCA(n_components=2)
&gt;&gt;&gt; print(pca.explained_variance_ratio_)
[0.9924... 0.0075...]
&gt;&gt;&gt; print(pca.singular_values_)
[6.30061... 0.54980...]
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; pca = PCA(n_components=2, svd_solver='full')
&gt;&gt;&gt; pca.fit(X)
PCA(n_components=2, svd_solver='full')
&gt;&gt;&gt; print(pca.explained_variance_ratio_)
[0.9924... 0.00755...]
&gt;&gt;&gt; print(pca.singular_values_)
[6.30061... 0.54980...]
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; pca = PCA(n_components=1, svd_solver='arpack')
&gt;&gt;&gt; pca.fit(X)
PCA(n_components=1, svd_solver='arpack')
&gt;&gt;&gt; print(pca.explained_variance_ratio_)
[0.99244...]
&gt;&gt;&gt; print(pca.singular_values_)
[6.30061...]
</code></pre></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="vectors.Word2Vec.get_word_vectors"><code class="name flex">
<span>def <span class="ident">get_word_vectors</span></span>(<span>words)</span>
</code></dt>
<dd>
<div class="desc"><p>converts a list of words into their word vectors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_word_vectors(words):
    &#34;&#34;&#34;
    converts a list of words into their word vectors
    &#34;&#34;&#34;
    return [nlp(word).vector for word in words]</code></pre>
</details>
</dd>
<dt id="vectors.Word2Vec.process_PCA"><code class="name flex">
<span>def <span class="ident">process_PCA</span></span>(<span>get_word_vectors)</span>
</code></dt>
<dd>
<div class="desc"><p>intialise pca model and tell it to project data down onto 2 dimensions</p>
<p>fit the pca model to our 300D data, this will work out which is the best
way to project the data down that will best maintain the relative distances
between data points. It will store these intructioons on how to transform the data.</p>
<p>Tell our (fitted) pca model to transform our 300D data down onto 2D using the
instructions it learnt during the fit phase.</p>
<p>let's look at our new 2D word vectors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def process_PCA(get_word_vectors):
    &#34;&#34;&#34;
    intialise pca model and tell it to project data down onto 2 dimensions

    fit the pca model to our 300D data, this will work out which is the best
    way to project the data down that will best maintain the relative distances
    between data points. It will store these intructioons on how to transform the data.

    Tell our (fitted) pca model to transform our 300D data down onto 2D using the
    instructions it learnt during the fit phase.

    let&#39;s look at our new 2D word vectors
    &#34;&#34;&#34;
    words = [&#39;car&#39;, &#39;truck&#39;, &#39;suv&#39;, &#39;race&#39;, &#39;elves&#39;, &#39;dragon&#39;, &#39;sword&#39;, &#39;king&#39;, &#39;queen&#39;, &#39;prince&#39;, &#39;horse&#39;, &#39;fish&#39;,
             &#39;lion&#39;, &#39;tiger&#39;, &#39;lynx&#39;, &#39;potato&#39;]

    # intialise pca model and tell it to project data down onto 2 dimensions
    pca = PCA(n_components=2)

    # fit the pca model to our 300D data, this will work out which is the best
    # way to project the data down that will best maintain the relative distances
    # between data points. It will store these intructioons on how to transform the data.
    pca.fit(get_word_vectors(words))

    # Tell our (fitted) pca model to transform our 300D data down onto 2D using the
    # instructions it learnt during the fit phase.
    word_vecs_2d = pca.transform(get_word_vectors(words))

    # let&#39;s look at our new 2D word vectors
    return word_vecs_2d</code></pre>
</details>
</dd>
<dt id="vectors.Word2Vec.similarity"><code class="name flex">
<span>def <span class="ident">similarity</span></span>(<span>a, b)</span>
</code></dt>
<dd>
<div class="desc"><p>Input two nlp() classes to determine similarity</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def similarity(a, b):
    &#34;&#34;&#34;
    Input two nlp() classes to determine similarity

    &#34;&#34;&#34;
    similarity = a.similarity(b)
    return similarity</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="vectors.Knn" href="#vectors.Knn">Knn</a></code></h4>
<ul class="">
<li><code><a title="vectors.Knn.density" href="#vectors.Knn.density">density</a></code></li>
<li><code><a title="vectors.Knn.fit" href="#vectors.Knn.fit">fit</a></code></li>
<li><code><a title="vectors.Knn.query" href="#vectors.Knn.query">query</a></code></li>
<li><code><a title="vectors.Knn.values" href="#vectors.Knn.values">values</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="vectors.KnnState" href="#vectors.KnnState">KnnState</a></code></h4>
<ul class="">
<li><code><a title="vectors.KnnState.NearestNeighbors" href="#vectors.KnnState.NearestNeighbors">NearestNeighbors</a></code></li>
<li><code><a title="vectors.KnnState.test" href="#vectors.KnnState.test">test</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="vectors.Notes" href="#vectors.Notes">Notes</a></code></h4>
<ul class="two-column">
<li><code><a title="vectors.Notes.Term_frequency" href="#vectors.Notes.Term_frequency">Term_frequency</a></code></li>
<li><code><a title="vectors.Notes.cosine_sim" href="#vectors.Notes.cosine_sim">cosine_sim</a></code></li>
<li><code><a title="vectors.Notes.distribution_plot" href="#vectors.Notes.distribution_plot">distribution_plot</a></code></li>
<li><code><a title="vectors.Notes.dtm_word_count" href="#vectors.Notes.dtm_word_count">dtm_word_count</a></code></li>
<li><code><a title="vectors.Notes.gather_data" href="#vectors.Notes.gather_data">gather_data</a></code></li>
<li><code><a title="vectors.Notes.test" href="#vectors.Notes.test">test</a></code></li>
<li><code><a title="vectors.Notes.tokenize" href="#vectors.Notes.tokenize">tokenize</a></code></li>
<li><code><a title="vectors.Notes.vectorize" href="#vectors.Notes.vectorize">vectorize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="vectors.Word2Vec" href="#vectors.Word2Vec">Word2Vec</a></code></h4>
<ul class="">
<li><code><a title="vectors.Word2Vec.PCA" href="#vectors.Word2Vec.PCA">PCA</a></code></li>
<li><code><a title="vectors.Word2Vec.get_word_vectors" href="#vectors.Word2Vec.get_word_vectors">get_word_vectors</a></code></li>
<li><code><a title="vectors.Word2Vec.process_PCA" href="#vectors.Word2Vec.process_PCA">process_PCA</a></code></li>
<li><code><a title="vectors.Word2Vec.similarity" href="#vectors.Word2Vec.similarity">similarity</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>